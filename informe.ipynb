{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b037b7",
   "metadata": {},
   "source": [

   # Primera parte

**a.** El Espacio Col(X) es el conjunto de todos los vectores de \(\mathbb{R}^n\) que pueden escribirse como combinación lineal de las columnas de X.

- ¿Col(X) es un subespacio vectorial de \(\mathbb{R}^n\)?  
- \(\mathrm{Col}(X) = \{\,b\in\mathbb{R}^n \mid b = X\beta \text{ con } \beta\in\mathbb{R}^p\}\)  
- ¿0 pertenece a Col(X)?

Consideremos el vector \(\beta = 0\) en \(\mathbb{R}^p\):  
\[
b = X\beta = X\cdot 0 = 0 \in \mathbb{R}^n
\]
por lo tanto \(0 \in \mathrm{Col}(X)\).

Dado \(b_1, b_2\in\mathrm{Col}(X)\), ¿\(b_1 + b_2\in\mathrm{Col}(X)\)?

Por definición existen \(\beta_1,\beta_2\in\mathbb{R}^p\) tales que:  
\[
b_1 = X\beta_1,\quad b_2 = X\beta_2.
\]
Verificamos la suma:  
\[
b_1 + b_2 = X\beta_1 + X\beta_2 = X(\beta_1 + \beta_2).
\]
Como \(\beta_1+\beta_2\in\mathbb{R}^p\) entonces \(b_1+b_2\in\mathrm{Col}(X)\).

Dado \(b\in\mathrm{Col}(X)\) y \(c\in\mathbb{R}\), ¿\(c\cdot b\in\mathrm{Col}(X)\)?

\[
b = X\beta \quad\Longrightarrow\quad c\,b = c\,(X\beta) = X(c\beta).
\]
Entonces \(c\cdot b\in\mathrm{Col}(X)\).

Verificamos que \(\mathrm{Col}(X)\) contiene al vector \(0\), \(b_1+b_2\) y \(c\,b\).  
Por lo tanto, se cumplen todas las condiciones para ser un subespacio de \(\mathbb{R}^n\).

---

**b.** Dados los vectores \(u, v\), queremos ver que \(u\cdot v = v\cdot u\).

\[
u \in \mathbb{R}^{n\times1},\quad v \in \mathbb{R}^{n\times1}.
\]

- \(u\cdot v = \sum_i u_i\,v_i\), declarado en la consigna.  
- \(v\cdot u = (v_1,\dots,v_n)\cdot u = \sum_i v_i\,u_i\).  
  En \(\mathbb{R}\) la multiplicación es conmutativa. Entonces:  
  \[
    u\cdot v = v\cdot u,
  \]
  lo que muestra que \(\sum_i u_i v_i = \sum_i v_i u_i\).

---

**c.** Definición de solución óptima:  
\[
\|y - X\beta^*\| = \min_{\beta\in\mathbb{R}^p} \|y - X\beta\|.
\]
Aplicamos el teorema de la proyección ortogonal:

1. Tomamos \(S = \mathrm{Col}(X)\), vector \(y\in\mathbb{R}^n\), vector en \(S\) \(b = X\beta^*\).  
2. El teorema afirma que minimizar la distancia equivale a  
   \((y - b)\cdot s = 0\), para todo \(s\in S\).  
3. Sustituimos \(b = X\beta^*\):  
   \[
     (y - X\beta^*)\cdot s = 0,\quad \forall s\in\mathrm{Col}(X).
   \]
4. Dado que cada \(s\in\mathrm{Col}(X)\) puede escribirse como \(s = X\beta\) con \(\beta\in\mathbb{R}^p\), la condición queda:  
   \[
     (y - X\beta^*)\cdot (X\beta) = 0.
   \]
Así logramos traducir la minimización de la norma \(\|y - X\beta^*\|\) en la condición de ortogonalidad.

---

**d.** Queremos llegar a:  
\[
X^\top (y - X\beta^*) = 0.
\]

- Partimos de la ecuación obtenida en (c):  
  \[
    (y - X\beta^*)\cdot (X\beta) = (X\beta)^\top (y - X\beta^*).
  \]
- Reescribimos:  
  \[
    (X\beta)^\top (y - X\beta^*) = \beta^\top X^\top (y - X\beta^*).
  \]
  Como \(\beta\) es un escalar, queda:  
  \[
    X^\top (y - X\beta^*) \cdot \beta.
  \]
- La condición (c) se transforma en:  
  \[
    (X\beta)^\top (y - X\beta^*) = 0
    \quad\Longrightarrow\quad
    X^\top (y - X\beta^*) = 0,
  \]
  que es la ecuación solicitada.

---

**e.** Queremos llegar a la fórmula de las ecuaciones normales:  
\[
X^\top X\,\beta^* = X^\top y.
\]

1. Partimos de \(X^\top (y - X\beta^*) = 0\).  
2. Definimos el vector \(u := X^\top (y - X\beta^*) \in \mathbb{R}^p\;\Rightarrow\;u=0\).  
3. En \(\mathbb{R}^p\), el único vector \(u\) que satisface \(u=0\) es el vector nulo.  
4. Entonces \(u=0\;\Rightarrow\;X^\top (y - X\beta^*)=0\;\Rightarrow\;X^\top y - X^\top X\,\beta^*=0\).  
5. Reordenamos:  
   \[
     X^\top X\,\beta^* = X^\top y,
   \]
   llegamos a la ecuación solicitada.

---

**f.** Queremos llegar a la fórmula de la solución óptima al problema de regresión:  
\[
\beta^* = (X^\top X)^{-1} X^\top y.
\]

1. Partimos de las ecuaciones normales: \(X^\top X\,\beta^* = X^\top y\).  
2. Suponemos que las columnas de \(X\) son linealmente independientes, por lo que \(X^\top X\) es invertible.  
3. Multiplicamos ambos lados por \((X^\top X)^{-1}\):  
   \[
     (X^\top X)^{-1} X^\top X\,\beta^* = (X^\top X)^{-1} X^\top y.
   \]
4. Como \((X^\top X)^{-1} X^\top X = I\), obtenemos:  
   \[
     \beta^* = (X^\top X)^{-1} X^\top y.
   \]
5. \(\beta^*\) es el vector de coeficientes que minimiza la suma de los cuadrados de los residuos.

    
   
   

  
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
